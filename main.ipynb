{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":316,"status":"ok","timestamp":1673600062481,"user":{"displayName":"Long Lê Ngọc","userId":"11020013756847842029"},"user_tz":-420},"id":"xTKiJIylVTJd"},"outputs":[],"source":["import tensorflow as tf\n","from IPython.display import clear_output\n","import os\n","# tf.multiply(2,3)\n","clear_output()"]},{"cell_type":"markdown","metadata":{"id":"0OqXj2ztVTJm"},"source":["> ## Customizing Models and Training Algorithms"]},{"cell_type":"markdown","metadata":{"id":"7pzgNpz5VTJp"},"source":[">> ### Custom Loss Functions"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1673600062826,"user":{"displayName":"Long Lê Ngọc","userId":"11020013756847842029"},"user_tz":-420},"id":"fkZBQABKVTJq"},"outputs":[],"source":["def huber_fn(y_true, y_pred):\n","    error = tf.math.subtract(y_true, y_pred)\n","    is_small_error = tf.abs(error) < 1\n","    squared_loss = tf.math.square(error) / 2\n","    linear_loss = tf.math.abs(error) - 0.5\n","\n","    return tf.where(is_small_error, squared_loss, linear_loss)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":283,"status":"ok","timestamp":1673600092526,"user":{"displayName":"Long Lê Ngọc","userId":"11020013756847842029"},"user_tz":-420},"id":"srmtkFfoVTJr"},"outputs":[],"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(30)\n","])\n","model.compile(loss=huber_fn, optimizer=\"nadam\")\n","clear_output()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Zet9fvkkVTJr"},"source":["### Saving and Loading Models That Contain Custom Components"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1673600094862,"user":{"displayName":"Long Lê Ngọc","userId":"11020013756847842029"},"user_tz":-420},"id":"DEPG5LGRVTJs"},"outputs":[],"source":["# model = tf.keras.models.save_model(\"my_model_with_a_custom_loss.h5\",\n","#                                     custom_objects={\"huber_fn\": huber_fn})"]},{"cell_type":"markdown","metadata":{"id":"_Im-yDOQVTJt"},"source":["> Create a subclass of the tf.keras.losses.Loss class, and then implementing its get_config() method to solve problem."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1673600099963,"user":{"displayName":"Long Lê Ngọc","userId":"11020013756847842029"},"user_tz":-420},"id":"z4tk24hCVTJu"},"outputs":[],"source":["class HuberLoss(tf.keras.losses.Loss):\n","    def __init__(self, threshold=1.0, **kwargs):\n","        self.threshold = threshold\n","        super().__init__(**kwargs)\n","\n","    def call(self, y_true, y_pred):\n","        error = tf.math.subtract(y_true - y_pred)\n","        is_small_error = tf.math.abs(error) < 1\n","        squared_loss = tf.math.square(error) / 2\n","        linear_loss = tf.math.multiply(self.threshold, tf.abs(error)) - tf.math.square(self.threshold) / 2\n","        return tf.where(is_small_error, squared_loss, linear_loss)\n","\n","    def get_config(self):\n","        base_config = super().get_config()\n","        return {**base_config, \"threshold\": self.threshold}"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":276,"status":"ok","timestamp":1673600137563,"user":{"displayName":"Long Lê Ngọc","userId":"11020013756847842029"},"user_tz":-420},"id":"kN1OkSO7VTJv"},"outputs":[],"source":["model.compile(loss=HuberLoss(2.), optimizer=\"nadam\")\n","model.build(input_shape=([]))"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":268,"status":"ok","timestamp":1673600139166,"user":{"displayName":"Long Lê Ngọc","userId":"11020013756847842029"},"user_tz":-420},"id":"VDNlHzD1Vjs2"},"outputs":[],"source":["model.save('my_model.h5')"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":274,"status":"ok","timestamp":1673600175477,"user":{"displayName":"Long Lê Ngọc","userId":"11020013756847842029"},"user_tz":-420},"id":"47MyWXuyVu6r"},"outputs":[],"source":["# load_model = tf.keras.models.load_model('my_model.h5',\n","#                                         custom_objects={\"HuberLoss\": HuberLoss})"]},{"cell_type":"markdown","metadata":{"id":"YfnvoS8lWFgW"},"source":[">> ### Custom Activation Functions, Initializers, Regularizers and Constraints"]},{"cell_type":"markdown","metadata":{"id":"9RVQhmfZWrOl"},"source":["> If a function has hyperparameters that need to be saved along with the model, then you will want to subclass the appropriate."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1673600483528,"user":{"displayName":"Long Lê Ngọc","userId":"11020013756847842029"},"user_tz":-420},"id":"tKQjKZSOWMiR"},"outputs":[],"source":["class MyL1Regularizer(tf.keras.regularizers.Regularizer):\n","  def __init__(self, factor):\n","    self.factor = factor\n","\n","  def __cal__(self, weights):\n","    return tf.reduce_sum(tf.abs(self.factor * weights))\n","\n","  def get_config(self):\n","    return {\"factor\": self.factor}"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"PMEva6_PXM8H"},"source":["### Custom Metrics"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"GDt0OTW7XLzJ"},"outputs":[],"source":["from tensorflow.python.ops.init_ops import Initializer\n","class HuberMetric(tf.keras.metrics.Metric):\n","  def __init__(self, threshold=1.0, **kwargs):\n","    super().__init__(**kwargs)   # Handle base arguments\n","    self.threshold = threshold\n","    self.huber_fn = self.create_huber(threshold)\n","    self.total = self.add_weight(\"total\", initializer=\"zeros\")\n","    self.count = self.add_weight(\"count\", initializer=\"zeros\")\n","\n","\n","  def update_state(self, y_true, y_pred, sample_weight=None):\n","    metric = self.huber_fn(y_true, y_pred)\n","    self.total.assign_add(tf.reduce_sum(metric))\n","    self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n","\n","  def result(self):\n","    return self.total / self.count\n","\n","  def get_config(self):\n","    base_config = super().get_config()\n","    return {**base_config, \"threshold\": self.threshold}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Custom Layers"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["class MyDense(tf.keras.layers.Layer):\n","    def __init__(self, units, activation=None, **kwargs):\n","        super().__init__(**kwargs)\n","        self.units = units\n","        self.activation = tf.keras.activations.get(activation)\n","\n","    def build(self, batch_input_shape):\n","        self.kernel = self.add_weight(\n","            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n","            initializer=\"glorot_normal\")\n","    \n","        self.bias = self.add_weight(\n","            name=\"bias\", shape=[self.units],\n","            initializer=\"zeros\")\n","\n","        super().build(batch_input_shape)        # must be at the end\n","\n","    def call(self, X):\n","        return self.activation(X @ self.kernel + self.bias)\n","\n","    def compute_output_shape(self, batch_input_shape):\n","        return tf.TensorShape(batch_input_shape.as_list()[:1] + [self.units])\n","\n","    def get_config(self):\n","        base_config = super().get_config()\n","        return {**base_config, \n","                \"units\": self.units,\n","                \"activation\": tf.keras.activations.serialize(self.activation)}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Custom Models"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["class ResidualBlock(tf.keras.layers.Layer):\n","    def __init__(self, n_layers, n_neurons, **kwargs):\n","        super().__init__(**kwargs)\n","        self.n_layers = n_layers\n","        self.n_neurons = n_neurons\n","        self.hiddden = [tf.keras.layers.Dense(self.n_neurons, \n","                                            activation=\"elu\",\n","                                            kernel_initializer=\"he_normal\")\n","                        for _ in range(self.n_layers)]\n","\n","        \n","        def call(self, inputs):\n","            x = inputs\n","            for layer in self.hidden:\n","                x = layer(x)\n","            return inputs + x\n","\n","        def get_config(self):\n","            base_config = super().get_config()\n","            return {**base_config,\n","                    \"n_layers\": self.n_layers,\n","                    \"n_neurons\": self.n_neurons}"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["class ResidualRegressor(tf.keras.Model):\n","    def __init__(self, output_dim, units, **kwargs):\n","        super().__init__(**kwargs)\n","        self.units = units\n","        self.output_dim = output_dim\n","        self.hidden1 = tf.keras.layers.Dense(units=self.units, activation=\"relu\")\n","        self.residualblock1 = ResidualBlock(2, self.units)\n","        self.residualblock2 = ResidualBlock(2, self.units)\n","        self.hidden2 = tf.keras.layers.Dense(units=self.output_dim)\n","\n","    def call(self, inputs):\n","        inputs = self.hidden1(inputs)\n","        for _ in range(1+3):\n","            inputs = self.residualblock1(inputs)\n","        inputs = self.residualblock2(inputs)\n","        inputs = self.hidden2(inputs)\n","        return inputs\n","\n","    def get_config(self):\n","        base_config = super().get_config()\n","        return {**base_config,\n","                \"units\": self.units,\n","                \"output_dim\": self.output_dim}\n","        "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Computing Gradients Using Autodiff"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def f(w1, w2):\n","    return tf.math.multiply(3, tf.math.square(w1)) + tf.math.multiply(2, w1 * w2)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n"," <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["w1, w2 = tf.Variable(5.), tf.Variable(3.)\n","with tf.GradientTape() as tape:\n","    z = f(w1, w2)\n","\n","gradients = tape.gradient(z, [w1, w2])\n","gradients"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["--------------- ERROR --------------\n","Use tape twice\n"]}],"source":["# tape is automatically erased immediately after you call its gradient() method => get error when call gradient() twice:\n","w1, w2 = tf.Variable(5.), tf.Variable(3.)\n","with tf.GradientTape() as tape:\n","    z = f(w1, w2)\n","\n","dz_dw1 = tape.gradient(z, w1)\n","try:\n","    dz_dw2 = tape.gradient(z, w2)   \n","except:\n","    print(\"--------------- ERROR --------------\")\n","    print(\"Use tape twice\")"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(36.0, shape=(), dtype=float32) tf.Tensor(10.0, shape=(), dtype=float32)\n"]}],"source":["# Solution: use persistent=True\n","w1, w2 = tf.Variable(5.), tf.Variable(3.)\n","with tf.GradientTape(persistent=True) as tape:\n","    z = f(w1, w2)\n","\n","dz_dw1 = tape.gradient(z, w1)\n","dz_dw2 = tape.gradient(z, w2) \n","print(dz_dw1, dz_dw2)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["---------- ERROR -------------\n","Not Variable\n","[None, None]\n"]}],"source":["# Defaul, tape only track operations involving variables\n","c1, c2 = tf.constant(5.), tf.constant(3.)\n","with tf.GradientTape() as tape:\n","    z = f(c1, c2)\n","\n","gradients = tape.gradient(z, [c1, c2])\n","print(\"---------- ERROR -------------\")\n","print(\"Not Variable\")\n","print(gradients)\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>, <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]\n"]}],"source":["# Solution: Force tape track object\n","with tf.GradientTape() as tape:\n","    tape.watch(c1)\n","    tape.watch(c2)\n","    z = f(c1, c2)\n","\n","gradients = tape.gradient(z, [c1, c2])\n","print(gradients)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/plain":["[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# stop gradient\n","\n","def f(w1, w2):\n","    return tf.math.multiply(3, tf.math.square(w1)) + tf.stop_gradient(tf.math.multiply(2, w1 * w2))\n","\n","with tf.GradientTape() as tape:\n","    z = f(w1, w2)\n","\n","tape.gradient(z, [w1, w2])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Custom Training Loops"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# build simple model\n","l2_reg = tf.keras.regularizers.l2(0.05)\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(30,\n","    activation='relu',\n","    kernel_initializer='he_normal',\n","    kernel_regularizer=l2_reg)\n","])"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# random sample batch\n","def random_batch(X, y, batch_size=32):\n","    import numpy as np\n","    idx = np.random.randint(len(X), size=batch_size)\n","    return X[idx], y[idx]"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# Function display the training status\n","def print_status_bar(iteration, total, loss, metrics=None):\n","    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result()) for m in [loss] + (metrics or [])])\n","    end = \"\" if iteration < total else \"\\n\"\n","    print(\"\\r{}/{} - \".format(iteration, total) + metrics, end=end)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# Define Hyper-paramater\n","X_train, y_train= tf.constant([1,2,3]), tf.constant([4,5,6])\n","n_epochs = 5\n","batch_size = 32\n","n_steps = len(X_train) // batch_size\n","optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n","loss_fn = tf.keras.losses.mean_absolute_error\n","mean_loss = tf.keras.metrics.Mean()\n","metrics = [tf.keras.metrics.MeanAbsoluteError()]"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","3/3 - mean: 0.0000 - mean_absolute_error: 0.0000\n","Epoch 2/5\n","3/3 - mean: 0.0000 - mean_absolute_error: 0.0000\n","Epoch 3/5\n","3/3 - mean: 0.0000 - mean_absolute_error: 0.0000\n","Epoch 4/5\n","3/3 - mean: 0.0000 - mean_absolute_error: 0.0000\n","Epoch 5/5\n","3/3 - mean: 0.0000 - mean_absolute_error: 0.0000\n"]}],"source":["# Training loop\n","for epoch in range(1, n_epochs + 1):\n","    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n","    for step in range(1, n_steps + 1):\n","        X_batch, y_batch = random_batch(X_train, y_train)\n","        with tf.GradientTape() as tape:\n","            y_pred = model(X_batch, training=True)\n","            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n","            loss = tf.add_n([main_loss] + model.losses)\n","        # Calculate Gradients\n","        gradients = tape.gradient(loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","        mean_loss(loss)\n","        for metric in metrics:\n","            metric(y_batch, y_pred)\n","        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n","    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n","    for metric in [mean_loss] + metrics:\n","        metric.reset_states()\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-2.4076061 , -1.407606  , -0.40760604], dtype=float32)>"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["y = tf.constant([1,2,3], dtype=tf.float32)\n","# calculate softmax\n","y_softmax = tf.nn.log_softmax(y)\n","y_softmax"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-2.4076061 , -1.407606  , -0.40760604], dtype=float32)>"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["a = tf.nn.log_softmax(y_softmax)\n","a"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-2.4076061 , -1.407606  , -0.40760604], dtype=float32)>"]},"execution_count":29,"metadata":{},"output_type":"execute_result"},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["tf.nn.log_softmax(a)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"c695a7c1db73d51cd02270fbcc6a5336958d25b354c79109e8794cb4835c69ab"}}},"nbformat":4,"nbformat_minor":0}
